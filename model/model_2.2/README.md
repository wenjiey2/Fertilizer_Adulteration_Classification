# Model 2.2

### Key features of Model 2.1 Network Architecture
- [x] Batch Norm
- [x] Adam optimizer with gradient clipping & label Smoothing for BCEL
- [x] Fold-dependent, serialized learning rate schedule -- warm up & exponential decay
- [x] Customize callback for early stopping
- [x] Mish Activation
- [x] Depthwise Separable Convolution (depthwise + pointwise)
- [x] Global Average Pooling in replacement of FC layers
- [x] Inception-v3 Module A, B with customzied residual connections
- [x] Data augmentation
- [x] Transfer learning with K-fold on randomized fill mode (deprecated in model 2.2)
- [x] Tensorboard visualization
- [x] Post-training weight quantization (float32, float16, int8)
- [x] YOLOv4 region of interest detection -- customized training & inference (darknet)
- [x] YOLOv4 region of interest detection -- inference & post-training quantization (tensorflow)
- [x] Cropping effectiveness analysis

### To-Do List of Model 2.2
- [x] Model Architecture redesign & refinement (for stable inference latency & accuracy)
- [x] Transfer learning on different fill mode without K-fold
- [x] Dataset Repreperation
- [x] Customized Per-layer Weight Pruning
- [x] Fine Tuning with Quantization Aware Training
- [ ] Selective Weight Clustering (currently not compatible with results from other optimizations)
- [x] Post-training quantization (currently dynamic quantization only)
- [x] Model deployment & runtime profiling for Inference on Andriod
- [x] Nonlinear classification based on subimages
- [x] Same DNN trained on a different dataset for fertilizer/non-fertilizer classification

### Summary of major updates of Model 2.2
1. Inference accuracy could vary a lot for Model 2.1 once images of different sizes are introduced, especially those of much smaller size (comparing to ~ 4000 by 3000). The density and size of the uera fertilizer could also vary depending on how a farmer distributes the fertilizer and takes the picture. Therefore, the dataset is ammended with carefully chosen lab photos taken from the previous semesters under much diversed and worse settings, so that the model can generalize better on images far from similar to our existing dataset.
2. There's also a considerable loss in image clarity when resizing them to 400 by 300 before the [partition](https://github.com/ACES-UIUC-Fertilizer-Group/Fertillizer_Adulteration_Detection_app/tree/master/model/model_2.1#dataset-preparation), which in particular can make certain pure fertilizer look blurry and more similar to adulterated ones. To resolve this issue, I maximize the size of the scaled image without compromising too much on training and inference latency. My chosen subimage size is 320 by 320, which means the raw image would be scaled to 1280 by 960.
3. Model optimization for size & latency with weight pruning, quantization & weight clustering. Notice that currently tensorflow is still not supportive of a few other speedup tricks yet, therefore, performance may be further improved in the future with the API update.
4. We introduce a new mechanism for the full image classification task based on subimage predictions. In replacement of linear thresholding of pure subimage counts, the overall idea is to use the prediction probabilities (of subimages being pure) generated by the DNN model and weight the probabilities for the center subimages more than the corner and edge ones.
5. Finally, we retain the key features of the DNN (only minor changes in fine tuning) and trained the DNN on a larger dataset consisting of a the entire fertilizer adulteration dataset and a subset of [ImageNet](https://www.image-net.org/).

### Training with your own images
You can train your own model on your PC for prediction if that is what's desired. With the current model complexity, you would need a GPU with at least 8GB memory. The training procedure remains mostly unchanged. Please also refer to Model 2.1 training specifics from links below.
#### [Dataset Preparation](https://github.com/ACES-UIUC-Fertilizer-Group/Fertillizer_Adulteration_Detection_app/tree/master/model/model_2.1#dataset-preparation)
The images are now scaled to 1280 by 960 and partitioned into 12 320 by 320 subimages, and k-folds training is deprecated in this update, therefore, you can manually select one of the 6 folds, rename it to *dataset* and move it into the */model_2.2* directory.

#### Training & Validation (refer to the previous training procedure [here](https://github.com/ACES-UIUC-Fertilizer-Group/Fertillizer_Adulteration_Detection_app/tree/master/model/model_2.1#training--validation))
- baseline_model.py - Baseline model reflects the updated DNN architecture redesign and basic training layout before optimization techniques are applied. If you have a vastly diverse and large dataset, it may be a good idea to train on the baseline model first.
- quantized_model.py - Applied per-layer quantization with customized bits on the baseline model for training.
- pruned_model.py - Applied pruning to the baseline/quantized model to zero out insignificant weights. In our trained model, I first trained on the pruned model and fine tuned it with quantization before pruning it again for the final model.
- clusted_model.py - Apply clustering to the final few layers for runtime optimization. It is currently not compatible with the result of the other two optimizations, and will compromise the accuracy by as large as a few percent.

#### [Testing on a new dataset](https://github.com/ACES-UIUC-Fertilizer-Group/Fertillizer_Adulteration_Detection_app/tree/master/model/model_2.1#testing-on-a-new-dataset)
- test.py - modified to test on any set of images it has not seen before with new mechanism on classification.

### Inference
Inference with different precision is no longer supported, because our target platform is an Andriod APP. Please refer to [here](https://github.com/ACES-UIUC-Fertilizer-Group/Fertillizer_Adulteration_Detection_app/tree/master/model/model_2.1#inference-with-different-precisions) if needed.
Currently, I use dynamic range for customized per-layer quantization for minimal accuracy loss. Also, int8 ops and uint8 quantization of input and outputs is supported in tensorflow 2.4.0 and above, currently we are still experimenting with them and experienced a significant accuracy drop.

**Our trained inference model can be found [here](https://drive.google.com/drive/folders/1UmpKOp49h79Y2rPL2AdQm6a2ht6zzffG?usp=sharing).**
The pruned_model.tflite is the file used in our app, the other two files are the .h5 model and weights.

Alternatively, you can train on Colab. You can request access to my Notebook [here](https://colab.research.google.com/drive/1363VdDVU10G5pVmapBSdDlWMduF4DsB5?usp=sharing). Notice that the entire procedure takes longer than the Colab timeout limit if you use the free service, so be aware of saving your training result. You can request access to my dataset for [fertilizer classification](https://drive.google.com/file/d/18PlaZeTpafMpnV3srGZgW5RTaE2SKPP1/view?usp=sharing) and [adulteration classification](https://drive.google.com/file/d/1-turbwaZ1frxOsrf1h-ohhNBuRGRonVl/view?usp=sharing).

### References
- [Batch Norm](https://arxiv.org/pdf/1502.03167.pdf)
- [LR Warmup](https://arxiv.org/pdf/1810.13243.pdf)
- [Mish Activation](https://arxiv.org/pdf/1908.08681.pdf)
- [Global Average Pooling](https://arxiv.org/pdf/1312.4400.pdf)
- [ResNet](https://arxiv.org/pdf/1512.03385.pdf)
- [Inception v3](https://arxiv.org/pdf/1512.00567.pdf)
- [Inception v4](https://arxiv.org/pdf/1602.07261.pdf)
- [Mobile Net](https://arxiv.org/pdf/1704.04861.pdf)
- [Quantization White Paper](https://arxiv.org/pdf/1806.08342.pdf)
- [Weight Pruning & Clustering](https://arxiv.org/pdf/1510.00149.pdf)

Lastly, if you have any questions, please contact Wenjie Yu.
